inference_type: "exllama" # exllama or ollama
inference_location: "remote" # remote or local
llm_path: "" # set only if inference_type is "exllama" and inference_location is "local" # this is actually model paramterer

filter_questions: ["Text", "Table"]
filter_questions_based_on: "pages" # categories or pages
replace_abbreviations: true
# if true then go grab the saved doc
inject_whitespace: false
# if true then go grab the saved doc

experiment_name: additional_pages

include_context: false # for generation
include_preceding_question_answers: true

max_new_tokens: 1024

# Retrieval
chunk_method: size # size, section, semantic
top_k: 5
optimization_method: # hypothetical_document # decomposing # paraphrasing # stepback # hypothetical_document
use_original_query_only: true
use_original_along_with_optimized: false

most_relevant_chunk_first: false
summarize_retrieved_documents: false
match_chunk_similarity_threshold: 95
chunk_size: 1024
surrounding_chunk_length: 0

# Set either of the following to true or both to false
reasoning: false
thinking: false