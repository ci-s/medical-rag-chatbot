inference_type: "exllama" # exllama or ollama
inference_location: "remote" # remote or local
llm_path: "" # set only if inference_type is "exllama" and inference_location is "local" # this is actually model paramterer

text_questions_only: true
replace_abbreviations: true
# if true then go grab the saved doc
inject_whitespace: true
# if true then go grab the saved doc

experiment_name: query_optimization

include_context: false # for generation
include_preceding_question_answers: true

max_new_tokens: 1024

# Retrieval
chunk_method: size # size, section, semantic
top_k: 5
optimization_method: decomposing # paraphrasing # stepback # hypothetical_document
use_original_query_only: false
use_original_along_with_optimized: false